{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2022 Semester 1\n",
    "\n",
    "## Assignment 2: Sentiment Classification of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zero_R(train_data, k_fold):\n",
    "    print(\"zero_R: \")\n",
    "    all_class = train_data[['sentiment']].values\n",
    "    class_list = []\n",
    "    for i in all_class:\n",
    "        if i[0] == 'negative':\n",
    "            class_list.append(2)\n",
    "        if i[0] == 'positive':\n",
    "            class_list.append(1)\n",
    "        if i[0] == 'neutral':\n",
    "            class_list.append(0)\n",
    "    all_text = train_data[['text']].values\n",
    "    kf = KFold(n_splits=k_fold,random_state=None)\n",
    "    acc_list = []\n",
    "    recall_list = []\n",
    "    preci_list = []\n",
    "    f1_list = []\n",
    "    fold = 1\n",
    "    # split data using 10-fold cross validation \n",
    "    for train_index , test_index in kf.split(all_text):\n",
    "        m = 0\n",
    "        X_train = []\n",
    "        X_test = []\n",
    "        y_train = []\n",
    "        y_test = []\n",
    "        for i in range(len(train_index)):\n",
    "            X_train.append(all_text[train_index[i]])\n",
    "            y_train.append(class_list[train_index[i]])\n",
    "        for i in range(len(test_index)):\n",
    "            X_test.append(all_text[test_index[i]])\n",
    "            y_test.append(class_list[test_index[i]])\n",
    "            \n",
    "        #count the majority of the class in each traing dataset\n",
    "        label_count = Counter(y_train)\n",
    "        major_label = label_count.most_common(1)[0][0]\n",
    "        label = np.full((len(test_index), 1), major_label, dtype = int)\n",
    "        predict_label = []\n",
    "        for i in range(len(label)):\n",
    "            predict_label.append(label[i][0])\n",
    "        confusion_matrix = metrics.confusion_matrix(y_test, predict_label, labels = [0,1,2])\n",
    "        accuracy = accuracy_score(y_test, predict_label)\n",
    "        precision = precision_score(y_test, predict_label, average = None,zero_division=1)\n",
    "        recall = recall_score(y_test, predict_label, average = None,zero_division=1)\n",
    "        f1_score = (2*precision*recall)/(recall+precision)\n",
    "        acc_list.append(accuracy)\n",
    "        preci_list.append(precision)\n",
    "        for i in precision:\n",
    "            if i == 1.0:\n",
    "                m = 1\n",
    "        #print result\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1_score)\n",
    "        print(\"fold \"+str(fold))\n",
    "        print(confusion_matrix)\n",
    "        print(\"Accuracy: \"+str(accuracy))\n",
    "        print(\"Precision: \"+str(precision))\n",
    "        print(\"Recall: \"+str(recall))\n",
    "        print(\"F1 score: \"+str(f1_score))\n",
    "        print('\\n')\n",
    "        fold += 1   \n",
    "    print('Average Accuracy with '+ str(k_fold) +' fold: '+ str(sum(acc_list)/k_fold))\n",
    "    if m != 1:\n",
    "        print('Average Precision with '+ str(k_fold) +' fold: '+ str(sum(preci_list)/k_fold))\n",
    "        print('Average Recall with '+ str(k_fold) +' fold: '+ str(sum(recall_list)/k_fold))\n",
    "        print('Average F1 score with '+ str(k_fold) +' fold: '+ str(sum(f1_list)/k_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split data using 10-fold cross validation\n",
    "def data_split(clean_text, class_label, k_fold):\n",
    "    all_text = clean_text\n",
    "    all_class = class_label\n",
    "    kf = KFold(n_splits=k_fold,random_state=None)\n",
    "    X_train_list = []\n",
    "    X_test_list = []\n",
    "    y_train_list = []\n",
    "    y_test_list = []\n",
    "    for train_index , test_index in kf.split(all_text):\n",
    "        m = 0\n",
    "        X_train = []\n",
    "        X_test = []\n",
    "        y_train = []\n",
    "        y_test = []\n",
    "        for i in range(len(train_index)):\n",
    "            X_train.append(all_text[train_index[i]])\n",
    "            y_train.append(all_class[train_index[i]])\n",
    "        for i in range(len(test_index)):\n",
    "            X_test.append(all_text[test_index[i]])\n",
    "            y_test.append(all_class[test_index[i]])\n",
    "        X_train_list.append(X_train)\n",
    "        X_test_list.append(X_test)\n",
    "        y_train_list.append(y_train)\n",
    "        y_test_list.append(y_test)\n",
    "    return X_train_list,X_test_list,y_train_list,y_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_R: \n",
      "fold 1\n",
      "[[1231    0    0]\n",
      " [ 556    0    0]\n",
      " [ 394    0    0]]\n",
      "Accuracy: 0.5644199908298946\n",
      "Precision: [0.56441999 1.         1.        ]\n",
      "Recall: [1. 0. 0.]\n",
      "F1 score: [0.72157093 0.         0.        ]\n",
      "\n",
      "\n",
      "fold 2\n",
      "[[1215    0    0]\n",
      " [ 563    0    0]\n",
      " [ 403    0    0]]\n",
      "Accuracy: 0.5570839064649243\n",
      "Precision: [0.55708391 1.         1.        ]\n",
      "Recall: [1. 0. 0.]\n",
      "F1 score: [0.7155477 0.        0.       ]\n",
      "\n",
      "\n",
      "fold 3\n",
      "[[1195    0    0]\n",
      " [ 574    0    0]\n",
      " [ 411    0    0]]\n",
      "Accuracy: 0.5481651376146789\n",
      "Precision: [0.54816514 1.         1.        ]\n",
      "Recall: [1. 0. 0.]\n",
      "F1 score: [0.70814815 0.         0.        ]\n",
      "\n",
      "\n",
      "fold 4\n",
      "[[1179    0    0]\n",
      " [ 583    0    0]\n",
      " [ 418    0    0]]\n",
      "Accuracy: 0.5408256880733945\n",
      "Precision: [0.54082569 1.         1.        ]\n",
      "Recall: [1. 0. 0.]\n",
      "F1 score: [0.70199464 0.         0.        ]\n",
      "\n",
      "\n",
      "fold 5\n",
      "[[1185    0    0]\n",
      " [ 613    0    0]\n",
      " [ 382    0    0]]\n",
      "Accuracy: 0.5435779816513762\n",
      "Precision: [0.54357798 1.         1.        ]\n",
      "Recall: [1. 0. 0.]\n",
      "F1 score: [0.70430906 0.         0.        ]\n",
      "\n",
      "\n",
      "fold 6\n",
      "[[1196    0    0]\n",
      " [ 583    0    0]\n",
      " [ 401    0    0]]\n",
      "Accuracy: 0.5486238532110091\n",
      "Precision: [0.54862385 1.         1.        ]\n",
      "Recall: [1. 0. 0.]\n",
      "F1 score: [0.70853081 0.         0.        ]\n",
      "\n",
      "\n",
      "fold 7\n",
      "[[1191    0    0]\n",
      " [ 577    0    0]\n",
      " [ 412    0    0]]\n",
      "Accuracy: 0.5463302752293578\n",
      "Precision: [0.54633028 1.         1.        ]\n",
      "Recall: [1. 0. 0.]\n",
      "F1 score: [0.70661525 0.         0.        ]\n",
      "\n",
      "\n",
      "fold 8\n",
      "[[1603    0    0]\n",
      " [ 341    0    0]\n",
      " [ 236    0    0]]\n",
      "Accuracy: 0.7353211009174312\n",
      "Precision: [0.7353211 1.        1.       ]\n",
      "Recall: [1. 0. 0.]\n",
      "F1 score: [0.84747555 0.         0.        ]\n",
      "\n",
      "\n",
      "fold 9\n",
      "[[1457    0    0]\n",
      " [ 460    0    0]\n",
      " [ 263    0    0]]\n",
      "Accuracy: 0.668348623853211\n",
      "Precision: [0.66834862 1.         1.        ]\n",
      "Recall: [1. 0. 0.]\n",
      "F1 score: [0.80120979 0.         0.        ]\n",
      "\n",
      "\n",
      "fold 10\n",
      "[[1207    0    0]\n",
      " [ 578    0    0]\n",
      " [ 395    0    0]]\n",
      "Accuracy: 0.5536697247706422\n",
      "Precision: [0.55366972 1.         1.        ]\n",
      "Recall: [1. 0. 0.]\n",
      "F1 score: [0.71272513 0.         0.        ]\n",
      "\n",
      "\n",
      "Average Accuracy with 10 fold: 0.580636628261592\n"
     ]
    }
   ],
   "source": [
    "#import all needed resources\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import Counter\n",
    "train_data = pd.read_csv(\"Train.csv\", sep=',')\n",
    "test_data = pd.read_csv(\"Test.csv\", sep=',')\n",
    "\n",
    "#implement zero R baseline\n",
    "zero_R(train_data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_text = train_data[['text']].values\n",
    "y = [x[0] for x in train_data[['sentiment']].values]\n",
    "clean_text_list = []\n",
    "for x in all_text:\n",
    "    text = re.sub(\"\\S*https?:\\S*\", \" \", x[0])\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    text = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    text = (\" \").join(ele for ele in tokens if ele.lower() not in stopwords.words('english'))\n",
    "    clean_text_list.append(text)\n",
    "clean_text = clean_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "X_train_list,X_test_list,y_train_list,y_test_list = data_split(clean_text, y, k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words\n",
      "Multinomial naive bayes\n",
      "Fold 1\n",
      "[[194 171  29]\n",
      " [190 847 194]\n",
      " [ 28 206 322]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.4709    0.4924    0.4814       394\n",
      "     neutral     0.6920    0.6881    0.6900      1231\n",
      "    positive     0.5908    0.5791    0.5849       556\n",
      "\n",
      "    accuracy                         0.6249      2181\n",
      "   macro avg     0.5846    0.5865    0.5854      2181\n",
      "weighted avg     0.6263    0.6249    0.6255      2181\n",
      "\n",
      "Fold 2\n",
      "[[213 168  22]\n",
      " [212 808 195]\n",
      " [ 25 236 302]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.4733    0.5285    0.4994       403\n",
      "     neutral     0.6667    0.6650    0.6658      1215\n",
      "    positive     0.5819    0.5364    0.5582       563\n",
      "\n",
      "    accuracy                         0.6066      2181\n",
      "   macro avg     0.5740    0.5767    0.5745      2181\n",
      "weighted avg     0.6091    0.6066    0.6073      2181\n",
      "\n",
      "Fold 3\n",
      "[[229 152  30]\n",
      " [175 813 207]\n",
      " [ 32 226 316]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.5252    0.5572    0.5407       411\n",
      "     neutral     0.6826    0.6803    0.6815      1195\n",
      "    positive     0.5714    0.5505    0.5608       574\n",
      "\n",
      "    accuracy                         0.6229      2180\n",
      "   macro avg     0.5931    0.5960    0.5943      2180\n",
      "weighted avg     0.6237    0.6229    0.6232      2180\n",
      "\n",
      "Fold 4\n",
      "[[208 172  38]\n",
      " [176 829 174]\n",
      " [ 25 229 329]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.5086    0.4976    0.5030       418\n",
      "     neutral     0.6740    0.7031    0.6883      1179\n",
      "    positive     0.6081    0.5643    0.5854       583\n",
      "\n",
      "    accuracy                         0.6266      2180\n",
      "   macro avg     0.5969    0.5884    0.5922      2180\n",
      "weighted avg     0.6247    0.6266    0.6252      2180\n",
      "\n",
      "Fold 5\n",
      "[[187 162  33]\n",
      " [175 807 203]\n",
      " [ 31 273 309]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.4758    0.4895    0.4826       382\n",
      "     neutral     0.6498    0.6810    0.6650      1185\n",
      "    positive     0.5670    0.5041    0.5337       613\n",
      "\n",
      "    accuracy                         0.5977      2180\n",
      "   macro avg     0.5642    0.5582    0.5604      2180\n",
      "weighted avg     0.5960    0.5977    0.5961      2180\n",
      "\n",
      "Fold 6\n",
      "[[204 170  27]\n",
      " [164 841 191]\n",
      " [ 23 229 331]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.5217    0.5087    0.5152       401\n",
      "     neutral     0.6782    0.7032    0.6905      1196\n",
      "    positive     0.6029    0.5678    0.5848       583\n",
      "\n",
      "    accuracy                         0.6312      2180\n",
      "   macro avg     0.6010    0.5932    0.5968      2180\n",
      "weighted avg     0.6293    0.6312    0.6300      2180\n",
      "\n",
      "Fold 7\n",
      "[[192 186  34]\n",
      " [165 835 191]\n",
      " [ 35 233 309]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.4898    0.4660    0.4776       412\n",
      "     neutral     0.6659    0.7011    0.6830      1191\n",
      "    positive     0.5787    0.5355    0.5563       577\n",
      "\n",
      "    accuracy                         0.6128      2180\n",
      "   macro avg     0.5781    0.5675    0.5723      2180\n",
      "weighted avg     0.6095    0.6128    0.6107      2180\n",
      "\n",
      "Fold 8\n",
      "[[126  89  21]\n",
      " [303 929 371]\n",
      " [ 16 130 195]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.2831    0.5339    0.3700       236\n",
      "     neutral     0.8092    0.5795    0.6754      1603\n",
      "    positive     0.3322    0.5718    0.4203       341\n",
      "\n",
      "    accuracy                         0.5734      2180\n",
      "   macro avg     0.4749    0.5618    0.4886      2180\n",
      "weighted avg     0.6777    0.5734    0.6024      2180\n",
      "\n",
      "Fold 9\n",
      "[[144 103  16]\n",
      " [290 881 286]\n",
      " [ 27 182 251]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.3124    0.5475    0.3978       263\n",
      "     neutral     0.7556    0.6047    0.6717      1457\n",
      "    positive     0.4539    0.5457    0.4956       460\n",
      "\n",
      "    accuracy                         0.5853      2180\n",
      "   macro avg     0.5073    0.5659    0.5217      2180\n",
      "weighted avg     0.6384    0.5853    0.6015      2180\n",
      "\n",
      "Fold 10\n",
      "[[179 186  30]\n",
      " [199 831 177]\n",
      " [ 26 229 323]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.4431    0.4532    0.4481       395\n",
      "     neutral     0.6669    0.6885    0.6775      1207\n",
      "    positive     0.6094    0.5588    0.5830       578\n",
      "\n",
      "    accuracy                         0.6115      2180\n",
      "   macro avg     0.5731    0.5668    0.5695      2180\n",
      "weighted avg     0.6111    0.6115    0.6109      2180\n",
      "\n",
      "Average Accuracy with 10 Fold: 0.6093013052677628\n",
      "Average Macro f1 score with 10 Fold: 0.5655838093963104\n"
     ]
    }
   ],
   "source": [
    "# define k frequent feature\n",
    "# if select is 1 then select k best feature before fitting the model\n",
    "max_features = 1000\n",
    "print('Bag of Words')\n",
    "print('Multinomial naive bayes')\n",
    "select = 0\n",
    "acc_list_multi = []\n",
    "macro_list_multi = []\n",
    "for i in range(k_fold):\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "    if select == 1:\n",
    "        bow = CountVectorizer()\n",
    "        X_train = bow.fit_transform(X_train)\n",
    "        X_test = bow.transform(X_test)\n",
    "        selector = SelectKBest(chi2, k=5000)\n",
    "        X_train = selector.fit_transform(X_train, y_train)\n",
    "        X_test = selector.transform(X_test)\n",
    "    else:\n",
    "        bow = CountVectorizer(max_features = max_features)\n",
    "        X_train = bow.fit_transform(X_train)\n",
    "        X_test = bow.transform(X_test)\n",
    "        \n",
    "    # fit model\n",
    "    classifier = MultinomialNB(alpha = 1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    classification_report = classification_report(y_test, y_pred,digits = 4)\n",
    "    acc_list_multi.append(accuracy_score(y_test, y_pred))\n",
    "    macro_list_multi.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    # print result\n",
    "    print('Fold '+str(i+1))\n",
    "    print(confusion_matrix)\n",
    "    print('\\nClassification Report')\n",
    "    print('======================================================')\n",
    "    print('\\n', classification_report)\n",
    "print('Average Accuracy with '+ str(k_fold) +' Fold: '+ str(sum(acc_list_multi)/k_fold))\n",
    "print('Average Macro f1 score with '+ str(k_fold) +' Fold: '+ str(sum(macro_list_multi)/k_fold))\n",
    "#max_features = 1000 average accuracy = 0.6093\n",
    "#best max_features = 2400 average accuracy = 0.6164\n",
    "#k = 1000 average accuracy = 0.6325\n",
    "#k = 2000 average accuracy = 0.6273\n",
    "#k = 3000 average accuracy = 0.6291\n",
    "#k = 5000 average accuracy = 0.6355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words\n",
      "SVM\n",
      "Fold 1\n",
      "[[  89  287   18]\n",
      " [  47 1125   59]\n",
      " [  15  297  244]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.5894    0.2259    0.3266       394\n",
      "     neutral     0.6583    0.9139    0.7653      1231\n",
      "    positive     0.7601    0.4388    0.5564       556\n",
      "\n",
      "    accuracy                         0.6685      2181\n",
      "   macro avg     0.6693    0.5262    0.5495      2181\n",
      "weighted avg     0.6718    0.6685    0.6328      2181\n",
      "\n",
      "Fold 2\n",
      "[[ 100  293   10]\n",
      " [  46 1079   90]\n",
      " [   6  333  224]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6579    0.2481    0.3604       403\n",
      "     neutral     0.6328    0.8881    0.7390      1215\n",
      "    positive     0.6914    0.3979    0.5051       563\n",
      "\n",
      "    accuracy                         0.6433      2181\n",
      "   macro avg     0.6607    0.5114    0.5348      2181\n",
      "weighted avg     0.6526    0.6433    0.6087      2181\n",
      "\n",
      "Fold 3\n",
      "[[  95  298   18]\n",
      " [  49 1061   85]\n",
      " [   7  333  234]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6291    0.2311    0.3381       411\n",
      "     neutral     0.6271    0.8879    0.7350      1195\n",
      "    positive     0.6944    0.4077    0.5137       574\n",
      "\n",
      "    accuracy                         0.6376      2180\n",
      "   macro avg     0.6502    0.5089    0.5289      2180\n",
      "weighted avg     0.6452    0.6376    0.6019      2180\n",
      "\n",
      "Fold 4\n",
      "[[ 105  299   14]\n",
      " [  49 1063   67]\n",
      " [  12  324  247]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6325    0.2512    0.3596       418\n",
      "     neutral     0.6305    0.9016    0.7421      1179\n",
      "    positive     0.7530    0.4237    0.5423       583\n",
      "\n",
      "    accuracy                         0.6491      2180\n",
      "   macro avg     0.6720    0.5255    0.5480      2180\n",
      "weighted avg     0.6637    0.6491    0.6153      2180\n",
      "\n",
      "Fold 5\n",
      "[[  94  271   17]\n",
      " [  50 1049   86]\n",
      " [  10  372  231]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6104    0.2461    0.3507       382\n",
      "     neutral     0.6200    0.8852    0.7292      1185\n",
      "    positive     0.6916    0.3768    0.4879       613\n",
      "\n",
      "    accuracy                         0.6303      2180\n",
      "   macro avg     0.6407    0.5027    0.5226      2180\n",
      "weighted avg     0.6384    0.6303    0.5950      2180\n",
      "\n",
      "Fold 6\n",
      "[[  81  304   16]\n",
      " [  53 1069   74]\n",
      " [  10  309  264]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.5625    0.2020    0.2972       401\n",
      "     neutral     0.6356    0.8938    0.7429      1196\n",
      "    positive     0.7458    0.4528    0.5635       583\n",
      "\n",
      "    accuracy                         0.6486      2180\n",
      "   macro avg     0.6479    0.5162    0.5345      2180\n",
      "weighted avg     0.6516    0.6486    0.6129      2180\n",
      "\n",
      "Fold 7\n",
      "[[  93  303   16]\n",
      " [  33 1080   78]\n",
      " [  14  334  229]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6643    0.2257    0.3370       412\n",
      "     neutral     0.6290    0.9068    0.7428      1191\n",
      "    positive     0.7090    0.3969    0.5089       577\n",
      "\n",
      "    accuracy                         0.6431      2180\n",
      "   macro avg     0.6674    0.5098    0.5295      2180\n",
      "weighted avg     0.6568    0.6431    0.6042      2180\n",
      "\n",
      "Fold 8\n",
      "[[  60  154   22]\n",
      " [ 118 1271  214]\n",
      " [   8  192  141]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.3226    0.2542    0.2844       236\n",
      "     neutral     0.7860    0.7929    0.7894      1603\n",
      "    positive     0.3740    0.4135    0.3928       341\n",
      "\n",
      "    accuracy                         0.6752      2180\n",
      "   macro avg     0.4942    0.4869    0.4889      2180\n",
      "weighted avg     0.6714    0.6752    0.6727      2180\n",
      "\n",
      "Fold 9\n",
      "[[  73  182    8]\n",
      " [ 114 1190  153]\n",
      " [   5  248  207]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.3802    0.2776    0.3209       263\n",
      "     neutral     0.7346    0.8167    0.7735      1457\n",
      "    positive     0.5625    0.4500    0.5000       460\n",
      "\n",
      "    accuracy                         0.6743      2180\n",
      "   macro avg     0.5591    0.5148    0.5315      2180\n",
      "weighted avg     0.6555    0.6743    0.6612      2180\n",
      "\n",
      "Fold 10\n",
      "[[  80  296   19]\n",
      " [  60 1072   75]\n",
      " [  12  323  243]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.5263    0.2025    0.2925       395\n",
      "     neutral     0.6339    0.8882    0.7398      1207\n",
      "    positive     0.7211    0.4204    0.5311       578\n",
      "\n",
      "    accuracy                         0.6399      2180\n",
      "   macro avg     0.6271    0.5037    0.5212      2180\n",
      "weighted avg     0.6375    0.6399    0.6034      2180\n",
      "\n",
      "Average Accuracy with 10 Fold: 0.6509948723125911\n",
      "Average Macro f1 score with 10 Fold: 0.5289343976683955\n"
     ]
    }
   ],
   "source": [
    "print('Bag of Words')\n",
    "print('SVM')\n",
    "max_features = 1000\n",
    "# select best feature if select = 1\n",
    "select = 0\n",
    "acc_list_svm = []\n",
    "macro_list_svm = []\n",
    "for i in range(k_fold):\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "    if select == 1:\n",
    "        bow = CountVectorizer()\n",
    "        X_train = bow.fit_transform(X_train)\n",
    "        X_test = bow.transform(X_test)\n",
    "        selector = SelectKBest(chi2, k=2000)\n",
    "        X_train = selector.fit_transform(X_train, y_train)\n",
    "        X_test = selector.transform(X_test)\n",
    "    else:\n",
    "        bow = CountVectorizer(max_features = max_features)\n",
    "        X_train = bow.fit_transform(X_train)\n",
    "        X_test = bow.transform(X_test)\n",
    "    classifier = svm.SVC(C=1.0,kernel = 'linear',decision_function_shape='ovr')\n",
    "    classifier.fit(X_train, y_train)\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    classification_report = classification_report(y_test, y_pred,digits = 4)\n",
    "    acc_list_svm.append(accuracy_score(y_test, y_pred))\n",
    "    macro_list_svm.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    print('Fold '+str(i+1))\n",
    "    print(confusion_matrix)\n",
    "    print('\\nClassification Report')\n",
    "    print('======================================================')\n",
    "    print('\\n', classification_report)\n",
    "print('Average Accuracy with '+ str(k_fold) +' Fold: '+ str(sum(acc_list_svm)/k_fold))\n",
    "print('Average Macro f1 score with '+ str(k_fold) +' Fold: '+ str(sum(macro_list_svm)/k_fold))\n",
    "#best max_features = 1000 average accuracy = 0.6510\n",
    "#max_features = 2400 average accuracy = 0.6482 \n",
    "#max_features = 3000 average accuracy = 0.6441\n",
    "#k = 5000 average accuracy = 0.6626\n",
    "#k = 3000 average accuracy = 0.6624\n",
    "#k = 2000 average accuracy = 0.6642\n",
    "#k = 1000 average accuracy = 0.6581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf\n",
      "multinomial naive bayes\n",
      "Fold 1\n",
      "[[  76  304   14]\n",
      " [  47 1122   62]\n",
      " [   3  365  188]]\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6032    0.1929    0.2923       394\n",
      "     neutral     0.6265    0.9115    0.7426      1231\n",
      "    positive     0.7121    0.3381    0.4585       556\n",
      "\n",
      "    accuracy                         0.6355      2181\n",
      "   macro avg     0.6473    0.4808    0.4978      2181\n",
      "weighted avg     0.6441    0.6355    0.5888      2181\n",
      "\n",
      "Fold 2\n",
      "[[  84  312    7]\n",
      " [  45 1120   50]\n",
      " [   3  380  180]]\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6364    0.2084    0.3140       403\n",
      "     neutral     0.6181    0.9218    0.7400      1215\n",
      "    positive     0.7595    0.3197    0.4500       563\n",
      "\n",
      "    accuracy                         0.6346      2181\n",
      "   macro avg     0.6713    0.4833    0.5013      2181\n",
      "weighted avg     0.6580    0.6346    0.5864      2181\n",
      "\n",
      "Fold 3\n",
      "[[  81  322    8]\n",
      " [  44 1097   54]\n",
      " [   3  389  182]]\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6328    0.1971    0.3006       411\n",
      "     neutral     0.6067    0.9180    0.7306      1195\n",
      "    positive     0.7459    0.3171    0.4450       574\n",
      "\n",
      "    accuracy                         0.6239      2180\n",
      "   macro avg     0.6618    0.4774    0.4920      2180\n",
      "weighted avg     0.6483    0.6239    0.5743      2180\n",
      "\n",
      "Fold 4\n",
      "[[  83  323   12]\n",
      " [  39 1093   47]\n",
      " [   2  402  179]]\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6694    0.1986    0.3063       418\n",
      "     neutral     0.6012    0.9271    0.7294      1179\n",
      "    positive     0.7521    0.3070    0.4361       583\n",
      "\n",
      "    accuracy                         0.6216      2180\n",
      "   macro avg     0.6742    0.4776    0.4906      2180\n",
      "weighted avg     0.6546    0.6216    0.5698      2180\n",
      "\n",
      "Fold 5\n",
      "[[  80  288   14]\n",
      " [  43 1087   55]\n",
      " [   6  437  170]]\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6202    0.2094    0.3131       382\n",
      "     neutral     0.5999    0.9173    0.7254      1185\n",
      "    positive     0.7113    0.2773    0.3991       613\n",
      "\n",
      "    accuracy                         0.6133      2180\n",
      "   macro avg     0.6438    0.4680    0.4792      2180\n",
      "weighted avg     0.6348    0.6133    0.5614      2180\n",
      "\n",
      "Fold 6\n",
      "[[  68  324    9]\n",
      " [  37 1098   61]\n",
      " [   3  387  193]]\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6296    0.1696    0.2672       401\n",
      "     neutral     0.6070    0.9181    0.7308      1196\n",
      "    positive     0.7338    0.3310    0.4563       583\n",
      "\n",
      "    accuracy                         0.6234      2180\n",
      "   macro avg     0.6568    0.4729    0.4847      2180\n",
      "weighted avg     0.6451    0.6234    0.5721      2180\n",
      "\n",
      "Fold 7\n",
      "[[  67  334   11]\n",
      " [  27 1111   53]\n",
      " [   4  384  189]]\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6837    0.1626    0.2627       412\n",
      "     neutral     0.6074    0.9328    0.7358      1191\n",
      "    positive     0.7470    0.3276    0.4554       577\n",
      "\n",
      "    accuracy                         0.6271      2180\n",
      "   macro avg     0.6794    0.4743    0.4846      2180\n",
      "weighted avg     0.6588    0.6271    0.5722      2180\n",
      "\n",
      "Fold 8\n",
      "[[  49  177   10]\n",
      " [  81 1336  186]\n",
      " [   3  230  108]]\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.3684    0.2076    0.2656       236\n",
      "     neutral     0.7665    0.8334    0.7986      1603\n",
      "    positive     0.3553    0.3167    0.3349       341\n",
      "\n",
      "    accuracy                         0.6849      2180\n",
      "   macro avg     0.4967    0.4526    0.4663      2180\n",
      "weighted avg     0.6591    0.6849    0.6683      2180\n",
      "\n",
      "Fold 9\n",
      "[[  53  204    6]\n",
      " [  87 1244  126]\n",
      " [   5  309  146]]\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.3655    0.2015    0.2598       263\n",
      "     neutral     0.7080    0.8538    0.7741      1457\n",
      "    positive     0.5252    0.3174    0.3957       460\n",
      "\n",
      "    accuracy                         0.6619      2180\n",
      "   macro avg     0.5329    0.4576    0.4765      2180\n",
      "weighted avg     0.6281    0.6619    0.6322      2180\n",
      "\n",
      "Fold 10\n",
      "[[  66  317   12]\n",
      " [  42 1103   62]\n",
      " [   2  387  189]]\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6000    0.1671    0.2614       395\n",
      "     neutral     0.6104    0.9138    0.7319      1207\n",
      "    positive     0.7186    0.3270    0.4495       578\n",
      "\n",
      "    accuracy                         0.6229      2180\n",
      "   macro avg     0.6430    0.4693    0.4809      2180\n",
      "weighted avg     0.6372    0.6229    0.5718      2180\n",
      "\n",
      "Average Accuracy with 10 Fold: 0.6348958688254273\n",
      "Average Macro f1 score with 10 Fold: 0.4854135370974225\n"
     ]
    }
   ],
   "source": [
    "print('tf')\n",
    "print('multinomial naive bayes')\n",
    "max_features = 1000\n",
    "select = 0\n",
    "acc_list = []\n",
    "macro_list = []\n",
    "for i in range(k_fold):\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "    if select == 1:\n",
    "        td = TfidfVectorizer()\n",
    "        X_train = td.fit_transform(X_train)\n",
    "        X_test = td.transform(X_test)\n",
    "        selector = SelectKBest(chi2, k=200)\n",
    "        X_train = selector.fit_transform(X_train, y_train)\n",
    "        X_test = selector.transform(X_test)\n",
    "    else:\n",
    "        td = TfidfVectorizer(max_features = max_features)\n",
    "        X_train = td.fit_transform(X_train)\n",
    "        X_test = td.transform(X_test)\n",
    "    classifier = MultinomialNB(alpha = 1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    classification_report = classification_report(y_test, y_pred,digits = 4)\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    acc_list.append(accuracy_score(y_test, y_pred))\n",
    "    macro_list.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    print('Fold '+str(i+1))\n",
    "    print(confusion_matrix)\n",
    "    print('Classification Report')\n",
    "    print('======================================================')\n",
    "    print('\\n', classification_report)\n",
    "print('Average Accuracy with '+ str(k_fold) +' Fold: '+ str(sum(acc_list)/k_fold))\n",
    "print('Average Macro f1 score with '+ str(k_fold) +' Fold: '+ str(sum(macro_list)/k_fold))\n",
    "#max_features = 1000 average accuracy = 0.6349\n",
    "#Best max_features = 2400 average accuracy = 0.6429\n",
    "#max_feature = 3000 average accuracy = 0.6416\n",
    "#k = 1000 average accuracy = 0.6325\n",
    "#k = 2000 average accuracy = 0.6273\n",
    "#k = 3000 average accuracy = 0.6291\n",
    "#k = 5000 average accuracy = 0.6355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf\n",
      "SVM\n",
      "Fold 1\n",
      "[[ 110  267   17]\n",
      " [  61 1098   72]\n",
      " [  13  291  252]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.5978    0.2792    0.3806       394\n",
      "     neutral     0.6630    0.8920    0.7607      1231\n",
      "    positive     0.7390    0.4532    0.5619       556\n",
      "\n",
      "    accuracy                         0.6694      2181\n",
      "   macro avg     0.6666    0.5415    0.5677      2181\n",
      "weighted avg     0.6706    0.6694    0.6413      2181\n",
      "\n",
      "Fold 2\n",
      "[[ 124  272    7]\n",
      " [  47 1089   79]\n",
      " [   2  336  225]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7168    0.3077    0.4306       403\n",
      "     neutral     0.6417    0.8963    0.7479      1215\n",
      "    positive     0.7235    0.3996    0.5149       563\n",
      "\n",
      "    accuracy                         0.6593      2181\n",
      "   macro avg     0.6940    0.5345    0.5645      2181\n",
      "weighted avg     0.6767    0.6593    0.6291      2181\n",
      "\n",
      "Fold 3\n",
      "[[ 110  285   16]\n",
      " [  54 1065   76]\n",
      " [   8  322  244]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6395    0.2676    0.3774       411\n",
      "     neutral     0.6370    0.8912    0.7429      1195\n",
      "    positive     0.7262    0.4251    0.5363       574\n",
      "\n",
      "    accuracy                         0.6509      2180\n",
      "   macro avg     0.6676    0.5280    0.5522      2180\n",
      "weighted avg     0.6609    0.6509    0.6196      2180\n",
      "\n",
      "Fold 4\n",
      "[[ 117  293    8]\n",
      " [  47 1059   73]\n",
      " [   5  328  250]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6923    0.2799    0.3986       418\n",
      "     neutral     0.6304    0.8982    0.7408      1179\n",
      "    positive     0.7553    0.4288    0.5470       583\n",
      "\n",
      "    accuracy                         0.6541      2180\n",
      "   macro avg     0.6927    0.5356    0.5622      2180\n",
      "weighted avg     0.6756    0.6541    0.6234      2180\n",
      "\n",
      "Fold 5\n",
      "[[ 113  253   16]\n",
      " [  45 1067   73]\n",
      " [   7  361  245]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6848    0.2958    0.4132       382\n",
      "     neutral     0.6347    0.9004    0.7446      1185\n",
      "    positive     0.7335    0.3997    0.5174       613\n",
      "\n",
      "    accuracy                         0.6537      2180\n",
      "   macro avg     0.6844    0.5320    0.5584      2180\n",
      "weighted avg     0.6713    0.6537    0.6226      2180\n",
      "\n",
      "Fold 6\n",
      "[[ 102  287   12]\n",
      " [  43 1078   75]\n",
      " [   7  315  261]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6711    0.2544    0.3689       401\n",
      "     neutral     0.6417    0.9013    0.7497      1196\n",
      "    positive     0.7500    0.4477    0.5607       583\n",
      "\n",
      "    accuracy                         0.6610      2180\n",
      "   macro avg     0.6876    0.5345    0.5597      2180\n",
      "weighted avg     0.6760    0.6610    0.6291      2180\n",
      "\n",
      "Fold 7\n",
      "[[ 124  275   13]\n",
      " [  30 1090   71]\n",
      " [   2  332  243]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.7949    0.3010    0.4366       412\n",
      "     neutral     0.6423    0.9152    0.7548      1191\n",
      "    positive     0.7431    0.4211    0.5376       577\n",
      "\n",
      "    accuracy                         0.6683      2180\n",
      "   macro avg     0.7268    0.5458    0.5764      2180\n",
      "weighted avg     0.6978    0.6683    0.6372      2180\n",
      "\n",
      "Fold 8\n",
      "[[  70  154   12]\n",
      " [ 105 1294  204]\n",
      " [   6  194  141]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.3867    0.2966    0.3357       236\n",
      "     neutral     0.7881    0.8072    0.7975      1603\n",
      "    positive     0.3950    0.4135    0.4040       341\n",
      "\n",
      "    accuracy                         0.6904      2180\n",
      "   macro avg     0.5233    0.5058    0.5124      2180\n",
      "weighted avg     0.6831    0.6904    0.6860      2180\n",
      "\n",
      "Fold 9\n",
      "[[  80  175    8]\n",
      " [ 112 1202  143]\n",
      " [   2  263  195]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.4124    0.3042    0.3501       263\n",
      "     neutral     0.7329    0.8250    0.7762      1457\n",
      "    positive     0.5636    0.4239    0.4839       460\n",
      "\n",
      "    accuracy                         0.6775      2180\n",
      "   macro avg     0.5696    0.5177    0.5367      2180\n",
      "weighted avg     0.6585    0.6775    0.6631      2180\n",
      "\n",
      "Fold 10\n",
      "[[ 102  282   11]\n",
      " [  55 1082   70]\n",
      " [   6  321  251]]\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.6258    0.2582    0.3656       395\n",
      "     neutral     0.6421    0.8964    0.7483      1207\n",
      "    positive     0.7560    0.4343    0.5516       578\n",
      "\n",
      "    accuracy                         0.6583      2180\n",
      "   macro avg     0.6746    0.5296    0.5552      2180\n",
      "weighted avg     0.6694    0.6583    0.6268      2180\n",
      "\n",
      "Average Accuracy with 10 Fold: 0.6642968464091467\n",
      "Average Macro f1 score with 10 Fold: 0.5545357744355743\n"
     ]
    }
   ],
   "source": [
    "print('tf')\n",
    "print('SVM')\n",
    "max_features = 3000\n",
    "select = 0\n",
    "acc_list_svm = []\n",
    "macro_list_svm = []\n",
    "for i in range(k_fold):\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "    if select == 1:\n",
    "        td = TfidfVectorizer()\n",
    "        X_train = td.fit_transform(X_train)\n",
    "        X_test = td.transform(X_test)\n",
    "        selector = SelectKBest(chi2, k=5000)\n",
    "        X_train = selector.fit_transform(X_train, y_train)\n",
    "        X_test = selector.transform(X_test)\n",
    "    else:\n",
    "        td = TfidfVectorizer(max_features = max_features)\n",
    "        X_train = td.fit_transform(X_train)\n",
    "        X_test = td.transform(X_test)\n",
    "    classifier = svm.SVC(C=1.0,kernel = 'linear',decision_function_shape='ovr')\n",
    "    classifier.fit(X_train, y_train)\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    classification_report = classification_report(y_test, y_pred,digits = 4)\n",
    "    acc_list_svm.append(accuracy_score(y_test, y_pred))\n",
    "    macro_list_svm.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    print('Fold '+str(i+1))\n",
    "    print(confusion_matrix)\n",
    "    print('\\nClassification Report')\n",
    "    print('======================================================')\n",
    "    print('\\n', classification_report)\n",
    "print('Average Accuracy with '+ str(k_fold) +' Fold: '+ str(sum(acc_list_svm)/k_fold))\n",
    "print('Average Macro f1 score with '+ str(k_fold) +' Fold: '+ str(sum(macro_list_svm)/k_fold))\n",
    "#max_features = 1000 average accuracy = 0.6509\n",
    "#max_features = 2400 average accuracy = 0.6622\n",
    "#max_features = 3000 average accuracy = 0.6643\n",
    "#k = 1000 average accuracy = 0.6531\n",
    "#k = 2000 average accuracy = 0.6577\n",
    "#k = 3000 average accuracy = 0.6592\n",
    "#k = 5000 average accuracy = 0.6612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict test.csv\n",
    "max_features = 3000\n",
    "X_train = clean_text\n",
    "all_text = test_data[['text']].values\n",
    "y_train = [x[0] for x in train_data[['sentiment']].values]\n",
    "clean_text_test = []\n",
    "for x in all_text:\n",
    "    text = re.sub(\"\\S*https?:\\S*\", \" \", x[0])\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    text = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    text = (\" \").join(ele for ele in tokens if ele.lower() not in stopwords.words('english'))\n",
    "    clean_text_test.append(text)\n",
    "X_test = clean_text_test\n",
    "td = TfidfVectorizer(max_features = max_features)\n",
    "X_train = td.fit_transform(X_train)\n",
    "X_test = td.transform(X_test)\n",
    "classifier = svm.SVC(C=1.0,kernel = 'linear',decision_function_shape='ovr')\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "predict_csv = pd.read_csv(\"Test.csv\", sep=',')\n",
    "predict_csv.drop('text', inplace=True, axis=1)\n",
    "predict_csv['sentiment'] = y_pred\n",
    "predict_csv.to_csv('prediction.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
